{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep-paraphrase-generation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graviraja/deep-paraphrase-generation/blob/master/deep_paraphrase_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_ur0LYsNLATW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **A deep generative framework for Paraphrase generation paper experimental code**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "rtgnfTK8Lhvq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will be using quora question pairs dataset \n",
        "\n",
        "I have downloaded dataset into google drive and will be using by mounting the drive"
      ]
    },
    {
      "metadata": {
        "id": "3PgppwmBK0ox",
        "colab_type": "code",
        "outputId": "c55086b8-8ba5-4884-ffed-2c8004a8a705",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/data')\n",
        "\n",
        "# if you want to upload the file, uncomment the following code\n",
        "# from google.colab import files\n",
        "# files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6qDAK-q8MXpW",
        "colab_type": "code",
        "outputId": "c6c268a1-6f65-4d18-9ac0-9ffc2d425c74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# if the file is uploaded, change the filename correspondingly\n",
        "filename = './data/My Drive/quora_questions.csv'\n",
        "filename"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./data/My Drive/quora_questions.csv'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "QTkd3kxtMrbI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "id": "cSGMAd2fMoMv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import math\n",
        "import spacy\n",
        "import random\n",
        "import codecs\n",
        "import collections\n",
        "import unicodedata\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TT0Kxx3EUTxL",
        "colab_type": "code",
        "outputId": "6fcb913c-074d-452b-d657-7c5a3db6a33a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "SEED = 5\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# use the gpu if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OXjBt4ZVNEZ2",
        "colab_type": "code",
        "outputId": "f5ddd969-d566-46e7-aaeb-5af697d140f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(filename, encoding='utf8')\n",
        "print(f\"Number of rows in the data: {len(df)}\")\n",
        "df.head()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows in the data: 404351\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>is_duplicate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "      <td>What would happen if the Indian government sto...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "      <td>How can Internet speed be increased by hacking...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "      <td>Which fish would survive in salt water?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  qid1  qid2                                          question1  \\\n",
              "0   0     1     2  What is the step by step guide to invest in sh...   \n",
              "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
              "2   2     5     6  How can I increase the speed of my internet co...   \n",
              "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
              "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
              "\n",
              "                                           question2  is_duplicate  \n",
              "0  What is the step by step guide to invest in sh...             0  \n",
              "1  What would happen if the Indian government sto...             0  \n",
              "2  How can Internet speed be increased by hacking...             0  \n",
              "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
              "4            Which fish would survive in salt water?             0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "ZKWkRi-7Pe1u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Considering only the duplicates, since they are sentences and paraphrases to each other"
      ]
    },
    {
      "metadata": {
        "id": "Y7-kAqQ_ONlr",
        "colab_type": "code",
        "outputId": "ec3a35fc-8915-46a5-9932-1c2250af3f79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "data = df[df['is_duplicate'] == 1]\n",
        "duplicate_df = data[['question1', 'question2']].reset_index(drop=True)\n",
        "print(f\"Number of duplicate questions: {len(data)}\")\n",
        "duplicate_df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of duplicate questions: 149306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
              "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How can I be a good geologist?</td>\n",
              "      <td>What should I do to be a great geologist?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How do I read and find my YouTube comments?</td>\n",
              "      <td>How can I see all my Youtube comments?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What can make Physics easy to learn?</td>\n",
              "      <td>How can you make physics easy to learn?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What was your first sexual experience like?</td>\n",
              "      <td>What was your first sexual experience?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           question1  \\\n",
              "0  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
              "1                     How can I be a good geologist?   \n",
              "2        How do I read and find my YouTube comments?   \n",
              "3               What can make Physics easy to learn?   \n",
              "4        What was your first sexual experience like?   \n",
              "\n",
              "                                           question2  \n",
              "0  I'm a triple Capricorn (Sun, Moon and ascendan...  \n",
              "1          What should I do to be a great geologist?  \n",
              "2             How can I see all my Youtube comments?  \n",
              "3            How can you make physics easy to learn?  \n",
              "4             What was your first sexual experience?  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "a3j5gSh-QDKl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Set the *sample_size* if you want to just try out. You can use the complete data when the model is built"
      ]
    },
    {
      "metadata": {
        "id": "YC7HaTuDPnsU",
        "colab_type": "code",
        "outputId": "48d2d5b1-ccfe-44f7-be90-102db56873c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "sample_size = 10000\n",
        "sample_data = duplicate_df[:sample_size]\n",
        "print(f\"Length of the sample data: {len(sample_data)}\")\n",
        "sample_data.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of the sample data: 10000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
              "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How can I be a good geologist?</td>\n",
              "      <td>What should I do to be a great geologist?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How do I read and find my YouTube comments?</td>\n",
              "      <td>How can I see all my Youtube comments?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What can make Physics easy to learn?</td>\n",
              "      <td>How can you make physics easy to learn?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What was your first sexual experience like?</td>\n",
              "      <td>What was your first sexual experience?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           question1  \\\n",
              "0  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
              "1                     How can I be a good geologist?   \n",
              "2        How do I read and find my YouTube comments?   \n",
              "3               What can make Physics easy to learn?   \n",
              "4        What was your first sexual experience like?   \n",
              "\n",
              "                                           question2  \n",
              "0  I'm a triple Capricorn (Sun, Moon and ascendan...  \n",
              "1          What should I do to be a great geologist?  \n",
              "2             How can I see all my Youtube comments?  \n",
              "3            How can you make physics easy to learn?  \n",
              "4             What was your first sexual experience?  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "5ivLqRnGQkPt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocessing the data"
      ]
    },
    {
      "metadata": {
        "id": "PuXXgUAMP9sZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def unicodeToAscii(s):\n",
        "    # convert the unicode format to ascii\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "\n",
        "def process_sentence(sentence):\n",
        "    # Lowercase, trim, and remove non-letter characters\n",
        "    s = unicodeToAscii(sentence.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z0-9.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R_QZEINQQ0Pr",
        "colab_type": "code",
        "outputId": "795d8f12-5324-4d47-f3f2-a6785b87c7fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "questions = [process_sentence(ques) for ques in sample_data['question1'].values]\n",
        "paraphrases = [process_sentence(ques) for ques in sample_data['question2'].values]\n",
        "print(questions[:5])\n",
        "print('----------------------------------')\n",
        "print(paraphrases[:5])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['astrology i am a capricorn sun cap moon and cap rising . . .what does that say about me ?', 'how can i be a good geologist ?', 'how do i read and find my youtube comments ?', 'what can make physics easy to learn ?', 'what was your first sexual experience like ?']\n",
            "----------------------------------\n",
            "['i m a triple capricorn sun moon and ascendant in capricorn what does this say about me ?', 'what should i do to be a great geologist ?', 'how can i see all my youtube comments ?', 'how can you make physics easy to learn ?', 'what was your first sexual experience ?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zH7AMOSxU2mC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Split the dataset into training and validation datasets"
      ]
    },
    {
      "metadata": {
        "id": "wRrtGhILU012",
        "colab_type": "code",
        "outputId": "321afd75-61e2-4f49-9a68-52b6ae345617",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "train_src, valid_src, train_trg, valid_trg = train_test_split(questions, paraphrases, test_size=0.2, random_state=SEED)\n",
        "print(f\"Length of training data: {len(train_src)}\")\n",
        "print(f\"Length of validation data: {len(valid_src)}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of training data: 8000\n",
            "Length of validation data: 2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T5m8zQP5SIdm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Creating the vocabulary\n",
        "\n",
        "We will use only the training data for creating the vocabulary"
      ]
    },
    {
      "metadata": {
        "id": "LNwAXgVsROtv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "  ''' Creates the vocabulary from the given corpus\n",
        "  \n",
        "  Args:\n",
        "    corpus: Data for the vocabulary needs to be constructed.\n",
        "  '''\n",
        "  def __init__(self, corpus):\n",
        "    \n",
        "    self.vocab_size = 0\n",
        "    self.word_to_idx = {}\n",
        "    self.idx_to_word = {}\n",
        "    \n",
        "    self.max_seq_len = 0\n",
        "    \n",
        "    self.pad_token = '<pad>'\n",
        "    self.go_token = '<go>'\n",
        "    self.end_token = '<eos>'\n",
        "    self.unk_token = '<unk>'\n",
        "    \n",
        "    self.build_vocab(corpus)\n",
        "    \n",
        "  def build_vocab(self, corpus):\n",
        "    # get the maximum sequence length from the given data\n",
        "    self.max_seq_len = max([len(sent) for sent in corpus])\n",
        "    \n",
        "    # build the vocab from the words\n",
        "    # you can use any other tokenizer here like spacy\n",
        "    self.build_word_vocab(corpus.split())\n",
        " \n",
        "  def build_word_vocab(self, words):\n",
        "    # create the word count\n",
        "    word_counts = collections.Counter(words)\n",
        "\n",
        "    # map index to words\n",
        "    idx_to_word = [x[0] for x in word_counts.most_common()]\n",
        "    self.idx_to_word = [self.pad_token, self.go_token, self.end_token, self.unk_token] + list(sorted(idx_to_word))\n",
        "\n",
        "    # vocab size\n",
        "    self.vocab_size = len(self.idx_to_word)\n",
        "\n",
        "    # map word to index\n",
        "    self.word_to_idx = {x: i for i, x in enumerate(self.idx_to_word)}\n",
        "  \n",
        "  def get_word_by_idx(self, idx):\n",
        "    # get the word of the given idx\n",
        "  \n",
        "    return self.idx_to_word[idx]\n",
        "  \n",
        "  def get_idx_by_word(self, word):\n",
        "    # get the idx of the given word if the word present in vocab, else return unk\n",
        "    \n",
        "    if word in self.word_to_idx.keys():\n",
        "      return self.word_to_idx[word]\n",
        "    else:\n",
        "      return self.word_to_idx[self.unk_token]\n",
        "  \n",
        "  def sample_word_from_distribution(self, distribution):\n",
        "    # randomly sample a word from the given distribution\n",
        "    \n",
        "    assert distribution.shape[-1] == self.vocab_size\n",
        "    ix = np.random.choice(range(self.vocab_size), p=distribution.ravel())\n",
        "    return self.idx_to_word[ix]\n",
        "\n",
        "  def likely_word_from_distribution(self, distribution):\n",
        "    # get the word which has max probability from the given distribution\n",
        "    \n",
        "    assert distribution.shape[-1] == self.vocab_size\n",
        "    ix = np.argmax(distribution.ravel())\n",
        "    return self.idx_to_word[ix]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QhjKUxKPUvA3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BatchLoader:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  def get_batch_data(self, batch_size, data_type='train'):\n",
        "    # get a data of batch_size from the data_type=train/valid\n",
        "    \n",
        "    if data_type == 'train':\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ueRtdFLMSW8q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Parameters of the model\n",
        "\n",
        "The following parameters class contains all the model related parameters"
      ]
    },
    {
      "metadata": {
        "id": "FhksWRlUSS_A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Parameters:\n",
        "  ''' This code contains the parameters of the model.\n",
        "  \n",
        "  Args:\n",
        "    max_seq_len: A integer indicating the max seq length\n",
        "    vocab_size: A integer indicating the vocab size\n",
        "    embedding_size: A integer indicating the word embedding size\n",
        "  '''\n",
        "  def __init__(self, max_seq_len, vocab_size, embedding_size):\n",
        "    \n",
        "    self.max_seq_len = int(max_seq_len) + 1\n",
        "    \n",
        "    self.vocab_size = int(vocab_size)\n",
        "    self.word_embed_size = embedding_size\n",
        "    \n",
        "    self.encoder_rnn_size = 600\n",
        "    self.encoder_num_layers = 1\n",
        "    \n",
        "    self.decoder_rnn_size = 600\n",
        "    self.decoder_num_layers = 2\n",
        "    \n",
        "    self.kld_penalty_weight = 1.0\n",
        "    self.cross_entropy_penalty_weight = 79.0\n",
        "  \n",
        "  def get_kld_coef(self, i):\n",
        "    # get the kl divergence penalty based on the iteration i\n",
        "   \n",
        "    return self.kld_penalty_weight * (math.tanh((i - 3500)/ 1000) + 1) / 2.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pa2GWq1lEtdu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Highway network\n",
        "\n",
        "This will be used before passing the input to encoder and decoder. So the embeddings are first passed through the highway network and then passed through the encoder/decoder."
      ]
    },
    {
      "metadata": {
        "id": "nG5MLH-gWnvq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Highway(nn.Module):\n",
        "  \"\"\"This code contains the implementation of Highwat network.\n",
        "  \n",
        "  \"\"\"\n",
        "  def __init__(self, size, num_layers, f):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.num_layers = num_layers\n",
        "    self.non_linear = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
        "    self.linear = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
        "    self.gate = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
        "    self.f = f\n",
        "  \n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x : input with shape of [batch_size, size]\n",
        "    Returns:\n",
        "        tensor with shape of [batch_size, size] after applying highway network\n",
        "    applies σ(x) ⨀ (f(G(x))) + (1 - σ(x)) ⨀ (Q(x)) transformation | G and Q is affine transformation,\n",
        "        f is non-linear transformation, σ(x) is affine transformation with sigmoid non-linearition\n",
        "        and ⨀ is element-wise multiplication\n",
        "    \"\"\"\n",
        "    for layer in range(self.num_layers):\n",
        "        # σ(x)\n",
        "        gate = F.sigmoid(self.gate[layer](x))\n",
        "\n",
        "        # f(G(x))\n",
        "        non_linear = self.f(self.nonlinear[layer](x))\n",
        "\n",
        "        # Q(x)\n",
        "        linear = self.linear[layer](x)\n",
        "\n",
        "        # σ(x) ⨀ (f(G(x))) + (1 - σ(x)) ⨀ (Q(x))\n",
        "        x = gate * non_linear + (1 - gate) * linear\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MldEk7xxY5m5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encoder\n",
        "\n",
        "Now let's see the encoder implementation. Our encoder contains 2 LSTM cells.\n",
        "\n",
        "\n",
        "*   First LSTM encoder takes the source sentence as input.\n",
        "*   Second LSTM encoder takes the paraphrase sentence as input along with first encoder's final hidden state as input\n",
        "*   After encoding the paraphrase sentence, we will create the latent parameters\n",
        "$\\mu , \\sigma$ from the final hidden state of the paraphrase encoder by passing through feed-forward layer\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "fsG4MkaSGWLW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  \"\"\" This code contains the encoder part of the paper.\n",
        "  \n",
        "  Args:\n",
        "    params: A Parameters instance which contains the model parameters details.\n",
        "    highway: A Highway network, applying for the input before passing to encoder.\n",
        "  \"\"\"\n",
        "  def __init__(self, params, highway):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.params = params\n",
        "    self.hw1 = highway\n",
        "    self.embedding = nn.Embedding(self.params.vocab_size, self.params.word_embed_size)\n",
        "    \n",
        "    # encoding rnns for the source and paraphrase\n",
        "    self.rnns = nn.ModuleList([\n",
        "        nn.LSTM(\n",
        "            input_size=self.params.word_embed_size,\n",
        "            hidden_size=self.params.encoder_rnn_size,\n",
        "            num_layers=self.params.encoder_num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True)\n",
        "        for _ in range(2)\n",
        "    ])\n",
        "    \n",
        "    # latent layers\n",
        "    # we will combine both hidden and cell states of the final layer of the bidirectional paraphrase lstm\n",
        "    self.context_to_mu = nn.Linear(self.params.encoder_rnn_size * 4, self.params.latent_variable_size)\n",
        "    self.context_to_logvar = nn.Linear(self.params.encoder_rnn_size * 4, self.params.latent_variable_size)\n",
        "  \n",
        "  def forward(self, input_source, input_paraphrase):\n",
        "    # input_source => [batch_size, inp_src_max_len]\n",
        "    # input_paraphrase => [batch_size, inp_par_max_len]\n",
        "    \n",
        "    # initial state for encoding the source sentence is zeros\n",
        "    # after encoding the source sentence, the state can be used to encode the paraphrase\n",
        "    state = None\n",
        "    for i, input in enumerate([input_source, input_paraphrase]):\n",
        "      # before passing the input to encoder rnn, pass it through the highway network\n",
        "      input = self.embedding(input)\n",
        "      [batch_size, seq_len, embedding_size] = input.size()\n",
        "      \n",
        "      input = input.view(-1, embedding_size)\n",
        "      input = self.hw1(input)\n",
        "      input = input.view(batch_size, seq_len, embedding_size)\n",
        "      \n",
        "      _, state = self.rnns[i](input, state)\n",
        "    \n",
        "    # final hidden, cell state after encoding the paraphrase\n",
        "    [h_state, c_state] = state\n",
        "    \n",
        "    # consider only the final layer's hidden state and cell state\n",
        "    h_state = h_state.view(self.params.encoder_num_layers, 2, batch_size, self.params.encoder_rnn_size)[-1]\n",
        "    c_state = c_state.view(self.params.encoder_num_layers, 2, batch_size, self.params.encoder_rnn_size)[-1]\n",
        "    \n",
        "    # make the hidden, cell states as batch major\n",
        "    h_state = h_state.permute(1, 0, 2).contiguous().view(batch_size, -1)\n",
        "    c_state = c_state.permute(1, 0, 2).contiguous().view(batch_size, -1)\n",
        "    \n",
        "    # concat the hidden and cell state\n",
        "    # final_state => [batch_size, encoder_rnn_size * 4]\n",
        "    final_state = torch.cat([h_state, c_state], -1)\n",
        "    \n",
        "    # latent parameters => [batch_size, latent_size]\n",
        "    mu = self.context_to_mu(final_state)\n",
        "    log_var = self.context_to_logvar(final_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DolVtiISgZ7p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Decoder\n",
        "\n",
        "Now let's see the decoder implementation. Our decoder contains 2 LSTMs.\n",
        "\n",
        "\n",
        "\n",
        "*   First LSTM is used for encoding the source sentence\n",
        "*   The second decoder is used for creating the paraphrase using the states from first encoder and latent vectors\n",
        "*   We will sample from latent vectors and pass along with word input as the input to the second decoder\n",
        "*   The output from the second decoder is passed through an output layer for predicting the word\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ZvYqxgCCY11e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, params, highway):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.params = params\n",
        "    self.hw1 = highway\n",
        "    self.embedding = nn.Embedding(self.params.vocab_size, self.params.word_embed_size)\n",
        "    \n",
        "    # encoding of the source sentence is bidirectional, since source is available at inference time\n",
        "    self.encoding_rnn = nn.LSTM(\n",
        "        input_size=self.params.word_embed_size,\n",
        "        hidden_size=self.params.encoder_rnn_size,\n",
        "        num_layers=self.params.encoder_num_layers,\n",
        "        batch_first=True,\n",
        "        bidirectional=True)\n",
        "    \n",
        "    # decoding of the paraphrase sentence, input is word concatenated with latent vector\n",
        "    self.decoding_rnn = nn.LSTM(\n",
        "        input_size=self.params.word_embed_size + self.params.latent_variable_size,\n",
        "        hidden_size=self.params.decoder_rnn_size,\n",
        "        num_layers=self.params.decoder_num_layers,\n",
        "        batch_first=True)\n",
        "    \n",
        "    # for converting the source decoder hidden state to hidden state of the paraphrase decoder\n",
        "    self.h_to_initial_state = nn.Linear(self.params.encoder_rnn_size * 2, self.params.decoder_num_layers * self.params.decoder_rnn_size)\n",
        "    \n",
        "    # for converting the source decoder cell state to cell state of the paraphrase decoder\n",
        "    self.c_to_initial_state = nn.Linear(self.params.encoder_rnn_size * 2, self.params.decoder_num_layers * self.params.decoder_rnn_size)\n",
        "    \n",
        "    # output layer\n",
        "    self.fc = nn.Linear(self.params.decoder_rnn_size, self.params.vocab_size)\n",
        "  \n",
        "  def build_initial_state(self, input):\n",
        "    # run the encoder rnn on the input to produce the hidden and cell state for paraphrase decoder\n",
        "    input = self.embedding(input)\n",
        "    [batch_size, seq_len, embed_size] = input.size()\n",
        "    input = input.view(-1, embed_size)\n",
        "    input = self.hw1(input)\n",
        "    input = input.view(batch_size, seq_len, embed_size)\n",
        "\n",
        "    _, cell_state = self.encoding_rnn(input)\n",
        "    [h_state, c_state] = cell_state\n",
        "\n",
        "    # consider only the final layer hidden and cell state\n",
        "    h_state = h_state.view(self.params.encoder_num_layers, 2, batch_size, self.params.encoder_rnn_size)[-1]\n",
        "    c_state = c_state.view(self.params.encoder_num_layers, 2, batch_size, self.params.encoder_rnn_size)[-1]\n",
        "\n",
        "    # convert to batch major format\n",
        "    h_state = h_state.permute(1, 0, 2).contiguous().view(batch_size, -1)\n",
        "    c_state = c_state.permute(1, 0, 2).contiguous().view(batch_size, -1)\n",
        "\n",
        "    # pass the hidden and cell state through a linear layer to get initial hidden and cell states for decoder\n",
        "    h_initial = self.h_to_initial_state(h_state).view(batch_size, self.params.decoder_num_layers, self.params.decoder_rnn_size)\n",
        "    # h_inital => [num_layers, batch_size, decoder_rnn_size]\n",
        "    h_initial = h_initial.permute(1, 0, 2).contiguous()\n",
        "\n",
        "    # pass the hidden and cell state through a linear layer to get initial hidden and cell states for decoder\n",
        "    c_initial = self.h_to_initial_state(c_state).view(batch_size, self.params.decoder_num_layers, self.params.decoder_rnn_size)\n",
        "    # c_inital => [num_layers, batch_size, decoder_rnn_size]\n",
        "    c_initial = c_initial.permute(1, 0, 2).contiguous()\n",
        "\n",
        "    return (h_initial, c_initial)\n",
        "  \n",
        "  def forward(self, encoder_input, decoder_input, z, drop_prob, initial_state=None):\n",
        "    # encoder_input => [batch_size, src_seq_len, embed_size]\n",
        "    # decoder_input => [batch_size, para_seq_len, embed_size]\n",
        "    # z is the latent variable shape => [batch_size, latent_variable_size]\n",
        "    # drop_prob is the probability of an element of decoder input to be zeroed in sense of dropout\n",
        "    # initial_state is the initial state of decoder\n",
        "    \n",
        "    if initial_state is None:\n",
        "      assert encoder_input is not None, \"Input should be provided to the encoder part of decoder\"\n",
        "      initial_state = self.build_initial_state(encoder_input)\n",
        "    \n",
        "    decoder_input = self.embedding(decoder_input)\n",
        "    [batch_size, seq_len, _] = decoder_input.size()\n",
        "    \n",
        "    # replicate z for seq_len times\n",
        "    z = torch.cat([z] * seq_len, 1).view(batch_size, seq_len, self.params.latent_variable_size)\n",
        "    \n",
        "    decoder_input = F.dropout(decoder_input, drop_prob)\n",
        "    \n",
        "    # concat the latent variable and word decoder input\n",
        "    decoder_input = torch.cat([decoder_input, z], 2)\n",
        "    # decoder_input => [batch_size, para_seq_len, embed_size + latent_variable_size]\n",
        "    \n",
        "    # decoder rnn\n",
        "    rnn_out, final_state = self.decoding_rnn(decoding_input, initial_state)\n",
        "    \n",
        "    # output layer\n",
        "    # rnn_out => [batch_size, para_seq_len, decoder_rnn_size]\n",
        "    rnn_out = rnn_out.contiguous().view(-1, self.params.decoder_rnn_size)\n",
        "    result = self.fc(rnn_out)\n",
        "    result = result.view(batch_size, seq_len, self.params.vocab_size)\n",
        "    \n",
        "    return result, final_state\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4gL6bFNIz0Xj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Paraphraser\n",
        "\n",
        "Now let's look into the Paraphraser implementation, which uses the Encoder and Decoder.\n",
        "\n",
        "We will implement the training part, evaluation part, inference code in the paraphraser."
      ]
    },
    {
      "metadata": {
        "id": "vzbmFGdEuUdo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Paraphraser(nn.Module):\n",
        "  def __init__(self, params, device):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.params = params\n",
        "    self.device = device\n",
        "    \n",
        "    # use 2 layers, relu activation for Highway\n",
        "    self.highway = Highway(self.params.word_embed_size, 2, F.relu)\n",
        "    self.encoder = Encoder(self.params, self.highway)\n",
        "    self.decoder = Decoder(self.params, self.highway)\n",
        "    \n",
        "  def forward(self, drop_prob, encoder_input=None, decoder_input=None, z=None, initial_state=None):\n",
        "    # encoder_input: A list of 2 tensors (original sentence, paraphrase sentence) with shape => [batch_size, seq_len]\n",
        "    # decoder_input: A list of 2 tensors (original sentence, paraphrase sentence) with shape => [batch_size, seq_len]\n",
        "    # initial_state: Initial state of decoder rnn in order to perform sampling\n",
        "    # drop_prob: probability of an element of decoder input to be zeroed in sense of dropout\n",
        "    # z: context if sampling\n",
        "    \n",
        "    if z is None:\n",
        "      # training case, get context from encoder and sample z ~ N(mu, std)\n",
        "      [batch_size, _] = encoder_input[0].size()\n",
        "      \n",
        "      mu, log_var = self.encoder(encoder_input[0], encoder_input[1])\n",
        "      std = torch.exp(0.5 * log_var)\n",
        "      \n",
        "      z = Variable(torch.randn([batch_size, self.params.latent_variable_size])).to(self.device)\n",
        "      z = z * std + mu\n",
        "      \n",
        "      # kl divergence loss\n",
        "      kld = (-0.5 * torch.sum(log_var - torch.pow(mu, 2) - torch.exp(log_var) + 1, 1)).mean().squeeze()\n",
        "    else:\n",
        "      kld = None\n",
        "    \n",
        "    out, final_state = self.decoder(decoder_input[0], decoder_input[1], z, drop_prob, initial_state)\n",
        "    \n",
        "    return out, final_state, kld\n",
        "  \n",
        "  def learnable_parameters(self):\n",
        "    return [p for p in self.parameters() if p.requires_grad]\n",
        "  \n",
        "  def trainer(self, optimizer, batch_loader):\n",
        "    def train(i, batch_size, dropout):\n",
        "      # i is the step number\n",
        "      # dropout to use in the decoder\n",
        "      \n",
        "      # get the batch data from training data\n",
        "      input = batch_loader.next_batch(batch_size, 'train')\n",
        "      input = [var.to(self.device) for var in input]\n",
        "      \n",
        "      [encoder_input_source, encoder_input_paraphrase, decoder_input_source, decoder_input_parapharse, target] = input\n",
        "      \n",
        "      # forward method\n",
        "      logits, _, kld = self(dropout, (encoder_input_source, encoder_input_paraphrase), (decoder_input_source, decoder), z=None)\n",
        "      \n",
        "      logits = logits.view(-1, self.params.vocab_size)\n",
        "      target = target.view(-1)\n",
        "      \n",
        "      cross_entropy_loss = F.cross_entropy(logits, target)\n",
        "      \n",
        "      # total loss is weighted reconstruction loss + weighted kl divergence loss\n",
        "      loss = self.params.cross_entropy_penalty_weight * cross_entropy_loss + self.params.get_kld_coef(i) * kld\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      return cross_entropy_loss, kld, self.params.get_kld_coef(i)\n",
        "      \n",
        "    return train\n",
        "  \n",
        "  def validator(self, batch_loader):\n",
        "    # we don't need to optimize the model during validation so optimizer not required\n",
        "    # batch_loader, which contains all the data related things\n",
        "    \n",
        "    def get_samples(logits, target):\n",
        "      # get the samples from logits\n",
        "      # logits => [batch_size, seq_len, vocab_size]\n",
        "      # target => [batch_size, seq_len]\n",
        "      \n",
        "      prediction = F.softmax(logits, dim=-1).data.cpu().numpy()\n",
        "      target = target.data.cpu().numpy()\n",
        "      \n",
        "      sampled, expected = [], []\n",
        "      for i in range(prediction.shape[0]):\n",
        "        sampled += [' '.join([batch_loader.sample_word_from_distribution(d) for d in prediction[i]])]\n",
        "        expected += [' '.join([batch_loader.get_word_by_idx(idx) for idx in target[i]])]\n",
        "      \n",
        "      return sampled, expected\n",
        "    \n",
        "    # validation loop of the model\n",
        "    def validate(batch_size, need_samples=False):\n",
        "      # batch size for the validation\n",
        "      # need_samples, where to have the sentences converted or not\n",
        "      if need_samples:\n",
        "        input, sentences = batch_loader.next_batch(batch_size, 'test', return_sentences=True)\n",
        "      else:\n",
        "        input = batch_loader.next_batch(batch_size, 'test')\n",
        "      \n",
        "      input = [var.to(self.device) for var in input]\n",
        "      \n",
        "      [encoder_input_source, encoder_input_paraphrase, decoder_input_source, decoder_input_paraphrase, target] = input\n",
        "      \n",
        "      # consider all the words during validation\n",
        "      logits, _, kld = self(0., (encoder_input_source, encoder_input_paraphrase), (decoder_input_source, decoder_input_paraphrase), z=None)\n",
        "      \n",
        "      if need_samples:\n",
        "        [s1, s2] = sentences\n",
        "        sampled, _ = get_samples(logits, target)\n",
        "      else:\n",
        "        s1, s2 = (None, None)\n",
        "        sampled, _ = (None, None)\n",
        "      \n",
        "      cross_entropy_loss = F.cross_entropy(logits, target)\n",
        "      return cross_entorpy_loss, kld, (sampled, s1, s2)\n",
        "    \n",
        "    return validate\n",
        "  \n",
        "  def sample_with_input(self, batch_loader, seq_len, use_mean, input):\n",
        "    [encoder_input_source, encoder_input_paraphrase, decoder_input_source, _, _] = input\n",
        "    \n",
        "    encoder_input = [encoder_input_source, encoder_input_paraphrase]\n",
        "    \n",
        "    mu, logvar = self.encoder(encoder_input[0], encoder_input[1])\n",
        "    std = torch.exp(0.5 * logvar)\n",
        "    \n",
        "    if use_mean:\n",
        "      z = mu\n",
        "    else:\n",
        "      z = Variable(torch.randn([batch_size, self.params.latent_variable_size])).to(self.device)\n",
        "      z = z * std + mu\n",
        "    \n",
        "    initial_state = self.decoder.build_initial_state(decoder_input_source)\n",
        "    \n",
        "    # initial input token to the decoder is the go label\n",
        "    decoder_input = batch_loader.get_raw_input_from_sentences([batch_loader.go_label])\n",
        "    \n",
        "    result = ''\n",
        "    \n",
        "    for i in range(seq_len):\n",
        "      decoder_input = decoder_input.to(self.device)\n",
        "      \n",
        "      logits, initial_state = self.decoder(None, decoder_input, z, 0.0, initial_state)\n",
        "      logits = logits.view(-1, self.params.vocab_size)\n",
        "      \n",
        "      prediction = F.softmax(logits, dim=-1)\n",
        "      word = batch_loader.likely_word_from_distribution(predicition.data.cpu().numpy()[-1])\n",
        "      \n",
        "      if word == batch_loader.end_label:\n",
        "        break\n",
        "      result += ' ' + word\n",
        "      \n",
        "      decoder_input = batch_loader.get_raw_input_from_sentences([word])\n",
        "    \n",
        "    return result\n",
        "      \n",
        "  def sample_with_pair(self, batch_loader, seq_len, source_sent, target_sent):\n",
        "    input = batch_loader.input_from_sentences([source_sent], [target_sent])\n",
        "    input = [var.to(self.device) for var in input]\n",
        "    \n",
        "    return self.sample_with_input(batch_loader, seq_len, False, input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M1NFIXlmuXy3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}